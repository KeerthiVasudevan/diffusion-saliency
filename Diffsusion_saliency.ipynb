{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KeerthiVasudevan/diffusion-saliency/blob/main/Diffsusion_saliency.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "6UbMscU15MkI"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import os\n",
        "from google.colab import files\n",
        "import kagglehub\n",
        "import torch.nn as nn\n",
        "from tqdm import tqdm\n",
        "import torch.nn.functional as F\n",
        "from torchvision import transforms\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hmvZS0UO543i",
        "outputId": "7bfba6d2-4f9d-4390-f9ee-e30b4123c555"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.10/dist-packages (1.6.17)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.10/dist-packages (from kaggle) (1.17.0)\n",
            "Requirement already satisfied: certifi>=2023.7.22 in /usr/local/lib/python3.10/dist-packages (from kaggle) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.8.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from kaggle) (4.66.6)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.10/dist-packages (from kaggle) (8.0.4)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.2.3)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from kaggle) (6.2.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->kaggle) (0.5.1)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.10/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (3.10)\n"
          ]
        }
      ],
      "source": [
        "!pip install kaggle\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "id": "XnSSFLWh7yXW",
        "outputId": "e3ddbc37-bbb4-4ea5-b2c9-c1c8b00c0027"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-8c34bd71-abe4-4281-8f05-4fcc1e08f71b\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-8c34bd71-abe4-4281-8f05-4fcc1e08f71b\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "{}"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LEzfILQ275ic",
        "outputId": "bab78177-1147-4b2e-a053-a89b6756e56d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘/root/.kaggle’: File exists\n",
            "cp: cannot stat 'kaggle.json': No such file or directory\n",
            "chmod: cannot access '/root/.kaggle/kaggle.json': No such file or directory\n"
          ]
        }
      ],
      "source": [
        "!mkdir ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WdaQ7HHL7ww-",
        "outputId": "ea97a63d-38b1-4aa6-f480-d0dff3d26a2f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Warning: Looks like you're using an outdated `kagglehub` version, please consider updating (latest version: 0.3.5)\n",
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/balraj98/duts-saliency-detection-dataset?dataset_version_number=1...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 392M/392M [00:03<00:00, 103MB/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Path to dataset files: /root/.cache/kagglehub/datasets/balraj98/duts-saliency-detection-dataset/versions/1\n"
          ]
        }
      ],
      "source": [
        "path = kagglehub.dataset_download(\"balraj98/duts-saliency-detection-dataset\")\n",
        "print(\"Path to dataset files:\", path)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aEJqfPPFzvDa"
      },
      "outputs": [],
      "source": [
        "train_path = os.path.join(path, \"DUTS-TR\")\n",
        "test_path = os.path.join(path, \"DUTS-TE\")\n",
        "train_images_path = os.path.join(train_path, \"DUTS-TR-Image\")\n",
        "train_saliency_path = os.path.join(train_path, \"DUTS-TR-Mask\")\n",
        "test_images_path = os.path.join(test_path, \"DUTS-TE-Image\")\n",
        "test_saliency_path = os.path.join(test_path, \"DUTS-TE-Mask\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5JNSPIosHdGr"
      },
      "outputs": [],
      "source": [
        "# Image Transform (for RGB images)\n",
        "image_transform = transforms.Compose([\n",
        "    transforms.Resize((128, 128)),   # Resize images to 128x128\n",
        "    transforms.ToTensor(),           # Convert to tensor (shape: [3, 128, 128])\n",
        "])\n",
        "\n",
        "# Saliency Transform (convert to 3 channels)\n",
        "saliency_transform = transforms.Compose([\n",
        "    transforms.Resize((128, 128)),                   # Resize saliency maps to 128x128\n",
        "    transforms.Grayscale(num_output_channels=3),     # Convert to 3 channels\n",
        "    transforms.ToTensor(),                           # Convert to tensor (shape: [3, 128, 128])\n",
        "])\n",
        "\n",
        "\n",
        "class SaliencyDataset(Dataset):\n",
        "  def __init__(self, image_dir, saliency_dir, image_transform, saliency_transform):\n",
        "    self.image_dir = image_dir\n",
        "    self.saliency_dir = saliency_dir\n",
        "    self.image_transform = image_transform\n",
        "    self.saliency_transform = saliency_transform\n",
        "    self.image_files = sorted(os.listdir(image_dir))\n",
        "    self.saliency_files = sorted(os.listdir(saliency_dir))\n",
        "    assert len(self.image_files) == len(self.saliency_files)\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.image_files)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    image_name = self.image_files[idx]\n",
        "    name_without_extension = os.path.splitext(image_name)[0]\n",
        "    saliency_name = name_without_extension + \".png\"\n",
        "    image_path = os.path.join(self.image_dir, self.image_files[idx])\n",
        "    # saliency_path = os.path.join(self.saliency_dir, self.saliency_files[idx])\n",
        "    saliency_path = os.path.join(self.saliency_dir, saliency_name)\n",
        "    # print(\"Index: \", idx)\n",
        "    try:\n",
        "      image = Image.open(image_path)\n",
        "      saliency = Image.open(saliency_path)\n",
        "    except FileNotFoundError:\n",
        "      print(f\"Image file not found: {image_path}\")\n",
        "      print(f\"Saliency file not found: {saliency_path}\")\n",
        "      return None\n",
        "    # image = Image.open(image_path)\n",
        "    # saliency = Image.open(saliency_path)\n",
        "    # if self.transform:\n",
        "    #   image = self.transform(image)\n",
        "    #   saliency = self.transform(saliency)\n",
        "\n",
        "    # Apply respective transforms\n",
        "    if self.image_transform:\n",
        "        image = self.image_transform(image)\n",
        "    if self.saliency_transform:\n",
        "        saliency = self.saliency_transform(saliency)\n",
        "\n",
        "    return image, saliency\n",
        "\n",
        "# Dataset and DataLoader\n",
        "dataset = SaliencyDataset(image_dir=train_images_path, saliency_dir=train_saliency_path, image_transform=image_transform, saliency_transform=saliency_transform)\n",
        "dataloader = DataLoader(dataset, batch_size=8, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zaosrtNnA1HG"
      },
      "outputs": [],
      "source": [
        "# UNET\n",
        "\n",
        "class TimeEmbedding(nn.Module):\n",
        "    def __init__(self, dim):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(dim, dim),\n",
        "            nn.SiLU(),\n",
        "            nn.Linear(dim, dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        half_dim = self.dim // 2\n",
        "        emb = torch.exp(torch.arange(half_dim, device=t.device) * -torch.log(torch.tensor(10000.0)) / (half_dim - 1))\n",
        "        emb = t[:, None] * emb[None, :]\n",
        "        emb = torch.cat((emb.sin(), emb.cos()), dim=-1)\n",
        "        return self.mlp(emb)\n",
        "\n",
        "\n",
        "class ConvBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(ConvBlock, self).__init__()\n",
        "        self.block = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.block(x)\n",
        "\n",
        "# Encoder Block: ConvBlock + MaxPooling\n",
        "class EncoderBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(EncoderBlock, self).__init__()\n",
        "        self.conv_block = ConvBlock(in_channels, out_channels)\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        features = self.conv_block(x)  # Extracted features\n",
        "        pooled = self.pool(features)   # Downsampled features\n",
        "        return features, pooled\n",
        "\n",
        "\n",
        "# Decoder Block: Upsampling + Skip Connection + ConvBlock\n",
        "class DecoderBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, time_dim):\n",
        "        super(DecoderBlock, self).__init__()\n",
        "        self.upsample = nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2)\n",
        "        self.conv_block = ConvBlock(out_channels * 2, out_channels)  # *2 for concatenation with skip features\n",
        "        self.time_mlp = nn.Linear(time_dim, out_channels)\n",
        "\n",
        "    def forward(self, x, skip_features, t_embed):\n",
        "        x = self.upsample(x)                           # Upsample\n",
        "        x = torch.cat([x, skip_features], dim=1)       # Concatenate skip features\n",
        "        # t_embed = self.time_mlp(t_embed).unsqueeze(-1).unsqueeze(-1)\n",
        "        # x = x + t_embed                                # Add timestep embedding\n",
        "\n",
        "        # Project time embedding to match x's channels\n",
        "        t_embed = self.time_mlp(t_embed)  # Shape: (batch_size, channels)\n",
        "        t_embed = t_embed.view(t_embed.shape[0], -1, 1, 1)  # Add spatial dimensions\n",
        "\n",
        "        # Dynamically match channels using a 1x1 Conv layer if needed\n",
        "        if t_embed.shape[1] != x.shape[1]:\n",
        "            t_embed = nn.Conv2d(t_embed.shape[1], x.shape[1], kernel_size=1).to(x.device)(t_embed)\n",
        "\n",
        "        # Add time embedding to x\n",
        "        x = x + t_embed\n",
        "        x = self.conv_block(x)                         # Apply convolutional block\n",
        "        return x\n",
        "\n",
        "\n",
        "class UNet(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, time_dim=256):\n",
        "        super(UNet, self).__init__()\n",
        "\n",
        "        self.time_embedding = TimeEmbedding(time_dim)\n",
        "        self.enc1 = EncoderBlock(in_channels, 64)\n",
        "        self.enc2 = EncoderBlock(64, 128)\n",
        "        self.enc3 = EncoderBlock(128, 256)\n",
        "        self.enc4 = EncoderBlock(256, 512)\n",
        "\n",
        "        self.bottleneck = ConvBlock(512, 1024)\n",
        "        self.bottleneck_time_embedding = nn.Linear(time_dim, 1024) # Timestep conditioning\n",
        "\n",
        "        # self.dec4 = DecoderBlock(1024, 512)\n",
        "        # self.dec3 = DecoderBlock(512, 256)\n",
        "        # self.dec2 = DecoderBlock(256, 128)\n",
        "        # self.dec1 = DecoderBlock(128, 64)\n",
        "\n",
        "        self.dec4 = DecoderBlock(1024, 512, time_dim)\n",
        "        self.dec3 = DecoderBlock(512, 256, time_dim)\n",
        "        self.dec2 = DecoderBlock(256, 128, time_dim)\n",
        "        self.dec1 = DecoderBlock(128, 64, time_dim)\n",
        "\n",
        "        self.final_conv = nn.Conv2d(64, out_channels, kernel_size=1)\n",
        "\n",
        "\n",
        "    # def forward(self, x):\n",
        "    #     enc1_features, enc1_pooled = self.enc1(x)\n",
        "    #     enc2_features, enc2_pooled = self.enc2(enc1_pooled)\n",
        "    #     enc3_features, enc3_pooled = self.enc3(enc2_pooled)\n",
        "    #     enc4_features, enc4_pooled = self.enc4(enc3_pooled)\n",
        "\n",
        "    #     bottleneck = self.bottleneck(enc4_pooled)\n",
        "\n",
        "    #     dec4 = self.dec4(bottleneck, enc4_features)\n",
        "    #     dec3 = self.dec3(dec4, enc3_features)\n",
        "    #     dec2 = self.dec2(dec3, enc2_features)\n",
        "    #     dec1 = self.dec1(dec2, enc1_features)\n",
        "    #     output = self.final_conv(dec1)\n",
        "\n",
        "    #     return output\n",
        "\n",
        "    def forward(self, x, t, image_features):\n",
        "        # Time embedding\n",
        "        t_embed = self.time_embedding(t)\n",
        "\n",
        "        # Encoder path\n",
        "        enc1_features, enc1_pooled = self.enc1(x + image_features)\n",
        "        enc2_features, enc2_pooled = self.enc2(enc1_pooled)\n",
        "        enc3_features, enc3_pooled = self.enc3(enc2_pooled)\n",
        "        enc4_features, enc4_pooled = self.enc4(enc3_pooled)\n",
        "\n",
        "        # Bottleneck with timestep conditioning\n",
        "        bottleneck = self.bottleneck(enc4_pooled) + self.bottleneck_time_embedding(t_embed).unsqueeze(-1).unsqueeze(-1)\n",
        "\n",
        "        # Decoder path with timestep\n",
        "        dec4 = self.dec4(bottleneck, enc4_features, t_embed)\n",
        "        dec3 = self.dec3(dec4, enc3_features, t_embed)\n",
        "        dec2 = self.dec2(dec3, enc2_features, t_embed)\n",
        "        dec1 = self.dec1(dec2, enc1_features, t_embed)\n",
        "        output = self.final_conv(dec1)\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# class UNet(nn.Module):\n",
        "#     def __init__(self, in_channels, out_channels):\n",
        "#         super(UNet, self).__init__()\n",
        "\n",
        "#     def encoder_block(self, in_channels, out_channels):\n",
        "#       x = self.conv_block(in_channels, out_channels)\n",
        "#       p = nn.MaxPool2d(kernel_size=2, stride=2)(x)\n",
        "#       return x, p\n",
        "\n",
        "\n",
        "\n",
        "#     def decoder_block(self, in_channels, out_channels, skip_features):\n",
        "#       x = nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2)(x)\n",
        "#       x = torch.cat([skip_features, x], dim=1)\n",
        "#       x = self.conv_block(out_channels * 2, out_channels)\n",
        "#       return x\n",
        "\n",
        "#     def forward(self, x):\n",
        "#       x1, p1 = self.encoder_block(x, 64)\n",
        "#       x2, p2 = self.encoder_block(p1, 128)\n",
        "#       x3, p3 = self.encoder_block(p2, 256)\n",
        "#       x4, p4 = self.encoder_block(p3, 512)\n",
        "#       x5 = self.conv_block(p4, 1024)\n",
        "#       d4 = self.decoder_block(x5, 512, x4)\n",
        "#       d3 = self.decoder_block(d5, 256, x3)\n",
        "#       d2 = self.decoder_block(d4, 128, x2)\n",
        "#       d1 = self.decoder_block(d3, 64, x1)\n",
        "#       output = nn.Conv2d(64, out_channels, kernel_size=1)(d1)\n",
        "#       return output\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nwaI6eJL_nbw"
      },
      "outputs": [],
      "source": [
        "# Diffusion model\n",
        "\n",
        "# class DiffusionModel(nn.Module):\n",
        "#     def __init__(self, unet):\n",
        "#         super(DiffusionModel, self).__init__()\n",
        "#         self.unet = unet\n",
        "\n",
        "#     def forward(self, noisy_map, t, condition):\n",
        "#         \"\"\"\n",
        "#         Parameters:\n",
        "#         - noisy_map: The noisy saliency map (input).\n",
        "#         - t: Current diffusion step.\n",
        "#         - condition: Image features (conditions for generation).\n",
        "#         \"\"\"\n",
        "#         denoised_map = self.unet(noisy_map, t, condition)\n",
        "#         return denoised_map\n",
        "\n",
        "class DiffusionSaliencyModel(nn.Module):\n",
        "    def __init__(self, unet, timesteps=1000, beta_start=0.0001, beta_end=0.02, device = \"cpu\"):\n",
        "        super(DiffusionSaliencyModel, self).__init__()\n",
        "        self.unet = unet\n",
        "        self.timesteps = timesteps\n",
        "\n",
        "        self.device = device\n",
        "        # Noise schedule\n",
        "        self.beta = torch.linspace(beta_start, beta_end, timesteps).to(device)\n",
        "        self.alpha = 1 - self.beta\n",
        "        self.alpha_bar = torch.cumprod(self.alpha, dim=0)\n",
        "\n",
        "    def add_noise(self, saliency_map, t):\n",
        "        \"\"\"\n",
        "        Add noise to the saliency map.\n",
        "        \"\"\"\n",
        "        t = t.to(torch.long).to(self.alpha_bar.device)\n",
        "        noise = torch.randn_like(saliency_map)\n",
        "        alpha_bar_t = self.alpha_bar[t].view(-1, 1, 1, 1)\n",
        "        # print(f\"t shape: {t.shape}\")\n",
        "\n",
        "        # print(f\"alpha_bar_t shape: {alpha_bar_t.shape}\")\n",
        "        # print(f\"saliency_map shape: {saliency_map.shape}\")\n",
        "        # print(f\"noise shape: {noise.shape}\")\n",
        "\n",
        "        noisy_map = torch.sqrt(alpha_bar_t) * saliency_map + torch.sqrt(1 - alpha_bar_t) * noise\n",
        "        return noisy_map, noise\n",
        "\n",
        "    def forward(self, saliency_map, image_features, t):\n",
        "      \"\"\"\n",
        "      Forward pass through the diffusion model.\n",
        "      Predict noise in the noisy saliency map.\n",
        "      \"\"\"\n",
        "      noisy_map, noise = self.add_noise(saliency_map, t)\n",
        "      predicted_noise = self.unet(noisy_map, t, image_features)\n",
        "      return F.mse_loss(predicted_noise, noise)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "3JnVrFpIAaDH",
        "outputId": "d8b4b229-3d10-4aec-b68c-b5074ff6293b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/1320 [00:00<?, ?it/s]<ipython-input-7-600333f160da>:53: UserWarning: Using a target size (torch.Size([8, 3, 128, 128])) that is different to the input size (torch.Size([8, 1, 128, 128])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(predicted_noise, noise)\n",
            "100%|█████████▉| 1319/1320 [01:10<00:00, 19.45it/s]<ipython-input-7-600333f160da>:53: UserWarning: Using a target size (torch.Size([1, 3, 128, 128])) that is different to the input size (torch.Size([1, 1, 128, 128])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(predicted_noise, noise)\n",
            "100%|██████████| 1320/1320 [01:10<00:00, 18.77it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1, Loss: 0.7154487371444702\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1320/1320 [01:09<00:00, 18.89it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2, Loss: 0.7070283889770508\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1320/1320 [01:09<00:00, 18.95it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3, Loss: 0.682328462600708\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1320/1320 [01:09<00:00, 19.10it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4, Loss: 0.9632171392440796\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1320/1320 [01:08<00:00, 19.38it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 5, Loss: 0.6935045123100281\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1320/1320 [01:09<00:00, 19.09it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 6, Loss: 0.6851201057434082\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1320/1320 [01:08<00:00, 19.17it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 7, Loss: 0.6818689703941345\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1320/1320 [01:08<00:00, 19.32it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 8, Loss: 0.6855558753013611\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1320/1320 [01:08<00:00, 19.25it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 9, Loss: 0.6920804977416992\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1320/1320 [01:09<00:00, 19.03it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 10, Loss: 0.6732673048973083\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Training\n",
        "\n",
        "unet = UNet(in_channels=3, out_channels=1)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "beta_start = 1e-6  # Very small initial noise\n",
        "beta_end = 1e-3    # Gradually increase to a small value\n",
        "\n",
        "# diffusion_model = DiffusionSaliencyModel(unet, device=device)\n",
        "diffusion_model = DiffusionSaliencyModel(unet, beta_start=beta_start, beta_end=beta_end, device=device)\n",
        "\n",
        "optimizer = torch.optim.Adam(diffusion_model.parameters(), lr=1e-4)\n",
        "diffusion_model.to(device)\n",
        "diffusion_model.train()\n",
        "for epoch in range(10):  # Number of epochs\n",
        "    for image, saliency_map in tqdm(dataloader):\n",
        "        if saliency_map is None:\n",
        "            continue\n",
        "        image, saliency_map = image.to(device), saliency_map.to(device)\n",
        "        # print(f\"t shape: {t.shape}\")  # Should be (batch_size,)\n",
        "        # print(f\"saliency_map shape: {saliency_map.shape}\")  # Should be (batch_size, 1, 128, 128)\n",
        "        # print(f\"image shape: {image.shape}\")  # Should be (batch_size, 3, 128, 128)\n",
        "\n",
        "        # t = torch.randint(0, diffusion_model.timesteps, (saliency_map.shape[0],)).to(device)\n",
        "        t = torch.randint(0, diffusion_model.timesteps, (saliency_map.shape[0],), dtype=torch.long).to(device)\n",
        "\n",
        "        loss = diffusion_model(saliency_map, image, t)\n",
        "\n",
        "        # Backpropagation\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}, Loss: {loss.item()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L05HBBoyLqCC"
      },
      "outputs": [],
      "source": [
        "# Test\n",
        "\n",
        "test_dataset = SaliencyDataset(image_dir=test_images_path, saliency_dir=test_saliency_path, image_transform=image_transform, saliency_transform=saliency_transform)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
        "\n",
        "def evaluate_model(model, dataloader, device):\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "    total_loss = 0\n",
        "    with torch.no_grad():  # Disable gradient calculations\n",
        "        for i, (images, saliency_maps) in enumerate(dataloader):\n",
        "            images = images.to(device)\n",
        "            saliency_maps = saliency_maps.to(device)\n",
        "\n",
        "            # Random timestep for testing\n",
        "            t = torch.randint(0, model.timesteps, (saliency_maps.shape[0],), dtype=torch.long).to(device)\n",
        "\n",
        "            # Get loss and predictions\n",
        "            loss = model(saliency_maps, images, t)\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            # Visualize a few examples\n",
        "            if i == 0:  # Only visualize the first batch\n",
        "                noisy_map, _ = model.add_noise(saliency_maps, t)\n",
        "                predicted_noise = model.unet(noisy_map, t, images)\n",
        "\n",
        "                # Convert predictions to saliency-like outputs\n",
        "                denoised_map = noisy_map - predicted_noise  # Simplified reverse step\n",
        "\n",
        "                # Plot\n",
        "                for j in range(min(4, images.size(0))):  # Show first 4 samples\n",
        "                    plt.figure(figsize=(10, 5))\n",
        "                    plt.subplot(1, 3, 1)\n",
        "                    plt.imshow(images[j].permute(1, 2, 0).cpu())  # Input image\n",
        "                    plt.title(\"Input Image\")\n",
        "                    plt.axis(\"off\")\n",
        "\n",
        "                    plt.subplot(1, 3, 2)\n",
        "                    plt.imshow(saliency_maps[j].squeeze().cpu(), cmap=\"gray\")  # Ground truth\n",
        "                    plt.title(\"Ground Truth Saliency\")\n",
        "                    plt.axis(\"off\")\n",
        "\n",
        "                    plt.subplot(1, 3, 3)\n",
        "                    plt.imshow(denoised_map[j].squeeze().cpu(), cmap=\"gray\")  # Predicted map\n",
        "                    plt.title(\"Predicted Saliency\")\n",
        "                    plt.axis(\"off\")\n",
        "\n",
        "                    plt.show()\n",
        "\n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "    print(f\"Average Test Loss: {avg_loss:.4f}\")\n",
        "\n",
        "evaluate_model(diffusion_model, test_dataloader, device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HV3Jqf7pwmhJ",
        "outputId": "20da0b87-9440-4005-a9db-492b426ca53e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "First 10 image and saliency maps\n"
          ]
        }
      ],
      "source": [
        "print(\"First 10 image and saliency maps\")\n",
        "image_files = sorted(os.listdir(test_images_path))\n",
        "saliency_map_files = sorted(os.listdir(test_saliency_path))\n",
        "for i in range(len(image_files)):\n",
        "  image_path = os.path.join(test_images_path, image_files[i])\n",
        "  saliency_map_path = os.path.join(test_saliency_path, saliency_map_files[i])\n",
        "  # check for jpg images\n",
        "  if image_path.endswith(\"03127747_6750.jpg\"):\n",
        "    print(\"image: \", image_path)\n",
        "    print(\"saliency map: \", saliency_map_path)\n",
        "  if saliency_map_path.endswith(\"03127747_6750.jpg\"):\n",
        "    print(\"image: \", image_path)\n",
        "    print(\"saliency map: \", saliency_map_path)\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": [],
      "authorship_tag": "ABX9TyM24odh209/sNmKpuPcbeLp",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}